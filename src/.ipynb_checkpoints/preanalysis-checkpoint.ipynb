{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from iq.uniqueness_metric import UniquenessMetric\n",
    "from nvd_entry import NVDEntry\n",
    "from solver.regexsolver import RegexSolver\n",
    "from utils import get_cve_list\n",
    "from downloader import Downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "Create the dataset using the `get_cve_list()` functionality in `src.utils`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2002, 2020))\n",
    "\n",
    "nvdfile_data = [NVDEntry(e) for year in range(2002, 2020) for e in get_cve_list(year)]\n",
    "valid_data = [e for e in nvdfile_data if not e.rejected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_db = defaultdict(list)\n",
    "for e in valid_data:\n",
    "    original_db[e.year].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = sum(len(original_db[y]) for y in original_db)\n",
    "    \n",
    "print(\"Total entries: \", total)\n",
    "print(original_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re = RegexSolver()\n",
    "solved_entries = re.solve(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solved_db = defaultdict(list)\n",
    "for e in solved_entries:\n",
    "    solved_db[e.year].append(e)\n",
    "assert(solved_db.keys() == original_db.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Regex-Path-Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/test.csv\", \"r\") as file:\n",
    "    id = 0\n",
    "    ids = []\n",
    "    for line in file:\n",
    "        splitted_line = line.split(\" \")\n",
    "        if splitted_line[2] != id and splitted_line[2] != \"\\n\" and splitted_line[2] != \"\":\n",
    "            id = splitted_line[2]\n",
    "            ids.append(id.replace(\"\\n\",\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"VP.txt\", \"w\")\n",
    "from flair.data import Sentence\n",
    "\n",
    "for id in ids:\n",
    "    found = False\n",
    "    path = []\n",
    "    for cve in solved_db[int(id[4:8])]:\n",
    "        if cve.id == id:\n",
    "            for description in cve.description:\n",
    "                sentence = Sentence(description, use_tokenizer=True)\n",
    "                if cve.path != None:\n",
    "                    path = \" \".join(cve.path)\n",
    "                    path = Sentence(path, use_tokenizer=True)\n",
    "                    path = [token.text for token in path]\n",
    "                    \n",
    "                path_cnt = 0\n",
    "                for token in (token.text for token in sentence):\n",
    "                    #if path_cnt < len(path) and token == path[path_cnt]:\n",
    "                    if token in path:\n",
    "                        tag = \"VP\"\n",
    "                        path_cnt += 1\n",
    "                    else:\n",
    "                        tag = \"O\"\n",
    "                    f.write(token + \" \" + tag + \" \" + cve.id + '\\n') \n",
    "                    tag = \"\"\n",
    "            f.write('\\n') \n",
    "            found = True\n",
    "            break\n",
    "    # Rejected are not found. Assuming the best case, that these 6 entries are tagged correctly\n",
    "    if not found:\n",
    "        f.write(\"** O CVE-2019-4782\\nREJECT O CVE-2019-4782\\n** O CVE-2019-4782\\nDO O CVE-2019-4782\\nNOT O CVE-2019-4782\\nUSE O CVE-2019-4782\\nTHIS O CVE-2019-4782\\nCANDIDATE O CVE-2019-4782\\nNUMBER O CVE-2019-4782\\n. O CVE-2019-4782\\nConsultIDs O CVE-2019-4782\\n: O CVE-2019-4782\\nnone O CVE-2019-4782\\n. O CVE-2019-4782\\nReason O CVE-2019-4782\\n: O CVE-2019-4782\\nThis O CVE-2019-4782\\ncandidate O CVE-2019-4782\\nwas O CVE-2019-4782\\nin O CVE-2019-4782\\na O CVE-2019-4782\\nCNA O CVE-2019-4782\\npool O CVE-2019-4782\\nthat O CVE-2019-4782\\nwas O CVE-2019-4782\\nnot O CVE-2019-4782\\nassigned O CVE-2019-4782\\nto O CVE-2019-4782\\nany O CVE-2019-4782\\nissues O CVE-2019-4782\\nduring O CVE-2019-4782\\n2019 O CVE-2019-4782\\n. O CVE-2019-4782\\nNotes O CVE-2019-4782\\n: O CVE-2019-4782\\nnone O CVE-2019-4782\\n. O CVE-2019-4782\\n\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/test.csv\", \"r\") as ground_truth_file:\n",
    "    vp_true_positive, vp_false_positive, vp_false_negative, vp_true_negative = 0,0,0,0\n",
    "    with open(\"VP.txt\", \"r\") as predicted_file:\n",
    "        predicted_lines = predicted_file.readlines()\n",
    "        for i, ground_truth_line in enumerate(ground_truth_file):\n",
    "            if ground_truth_line.replace(\"\\n\", \"\").replace(\" \", \"\") == \"\":\n",
    "                continue\n",
    "            predicted_line = predicted_lines[i]\n",
    "            predicted_tag = predicted_line.split(\" \")[1]\n",
    "            \n",
    "            if predicted_tag == \"VP\" and \"VP\" in ground_truth_line.split(\" \")[1]:\n",
    "                vp_true_positive += 1\n",
    "            elif predicted_tag == \"VP\":\n",
    "                vp_false_positive += 1\n",
    "            elif predicted_tag == \"O\" and \"VP\" in ground_truth_line.split(\" \")[1]:\n",
    "                vp_false_negative += 1\n",
    "            else:\n",
    "                vp_true_negative += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vp_true_positive)\n",
    "print(vp_false_positive)\n",
    "print(vp_false_negative)\n",
    "print(vp_true_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vp_prec = vp_true_positive/(vp_false_positive + vp_true_positive)\n",
    "vp_rec = vp_true_positive/(vp_true_positive + vp_false_negative)\n",
    "vp_f1 = 2 * vp_prec * vp_rec / (vp_prec + vp_rec)\n",
    "\n",
    "print(vp_prec)\n",
    "print(vp_rec)\n",
    "print(vp_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preanalyse Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_cpe = [entry for y in original_db for entry in original_db[y] if len(entry.cpe) == 0]\n",
    "print(\"Affected by problem IV: \", len(no_cpe))\n",
    "print(no_cpe[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num entries', len(valid_data))\n",
    "\n",
    "# Problem I    \n",
    "no_cwe = [entry for entry in valid_data if len(entry.cwe) == 0]\n",
    "cwe_other = [entry for entry in valid_data if len(entry.cwe) > 0 and any('NVD-CWE' in e for e in entry.cwe)]\n",
    "print(\"No cwe (pI)\\t \", len(cwe_other) + len(no_cwe))\n",
    "\n",
    "# Problem II\n",
    "old_cvss = [entry for entry in valid_data if not entry.cvssv3]\n",
    "new_cvss = [entry for entry in valid_data if entry.cvssv3]\n",
    "print('Old cvss (pII)\\t ', len(old_cvss))\n",
    "\n",
    "# Problem III\n",
    "path_found = [entry for entry in solved_entries if entry.path or entry.vulnerable_function]\n",
    "print('Path found (pIII)', len(path_found))\n",
    "print([e.path for e in solved_entries if e.path][:100])\n",
    "\n",
    "# Problem IV\n",
    "no_cpe = [entry for entry in valid_data if not len(entry.cpe)]\n",
    "print(\"No cpe (pIV)\\t \", len(no_cpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_year_clusters = dict()\n",
    "solved_year_clusters = dict()\n",
    "original_scores_by_year = dict()\n",
    "solved_scores_by_year = dict()\n",
    "\n",
    "um = UniquenessMetric(['cwe', 'cpe', 'cvssv3', 'path'])\n",
    "\n",
    "for year in original_db.keys():\n",
    "    original_year_clusters[year] = um.build_clusters(original_db[year])\n",
    "    solved_year_clusters[year] = um.build_clusters(solved_db[year])\n",
    "    original_scores_by_year[year] = um.score(original_db[year])\n",
    "    solved_scores_by_year[year] = um.score(solved_db[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(original_year_clusters.keys())\n",
    "year = next(iter(original_year_clusters))\n",
    "\n",
    "print(list(original_year_clusters[year].keys())[:5]) # hashes\n",
    "single_hash = next(iter(original_year_clusters[year]))\n",
    "\n",
    "cluster = original_year_clusters[year][single_hash]\n",
    "print(len(cluster))\n",
    "print(type(cluster[0]))\n",
    "print(cluster[0].id)\n",
    "#for c in original_year_clusters[year].values():\n",
    "#    print(len(c))\n",
    "    \n",
    "print(list(original_scores_by_year.items()))\n",
    "print(list(solved_scores_by_year.items()))\n",
    "print(list(zip(original_scores_by_year.values(), solved_scores_by_year.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clusters by Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def cluster_by_size(year_clusters: dict):\n",
    "    clusters_by_size_dict = collections.defaultdict(list)\n",
    "    for cluster in year_clusters.values():\n",
    "        clusters_by_size_dict[len(cluster)].append(cluster)\n",
    "    return clusters_by_size_dict\n",
    "\n",
    "original_cluster_size_dict = {year:cluster_by_size(original_year_clusters[year]) for year in original_year_clusters}\n",
    "solved_cluster_size_dict = {year:cluster_by_size(solved_year_clusters[year]) for year in solved_year_clusters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "year = random.randint(2002, 2019)\n",
    "\n",
    "print(sorted(original_cluster_size_dict.keys()))\n",
    "\n",
    "print(year)\n",
    "print(sorted(original_cluster_size_dict[year].keys()))\n",
    "for size in sorted(original_cluster_size_dict[year]):\n",
    "    print(f'{size:2}: {len(original_cluster_size_dict[year][size]):5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = -1\n",
    "\n",
    "ids = [entry.id for entry in original_cluster_size_dict[year][sorted(original_cluster_size_dict[year].keys())[tmp]][0]]\n",
    "print(len(ids), ', '.join(ids))\n",
    "original_cluster_size_dict[year][sorted(original_cluster_size_dict[year].keys())[tmp]][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings of manual analysis of the cluster in 2019:\n",
    "\n",
    "- Some descriptions contain errors: Android ID should be Android Bug ID (e.g., CVE-2019-2061 until CVE-2019-2078)\n",
    "- Libraries are accounted for as part of android, while they should be products on their own (e.g. libvpx, now owned by google has cpe ID `cpe:2.3:o:google:android:10.0:*:*:*:*:*:*:*` while there exists `cpe:2.3:a:webmproject:libvpx:-:*:*:*:*:*:*:*` [1]. Introduction of `cpe:2.3:a:google:libvpx:-:*:*:*:*:*:*:*` with the according version could resolve these clusters (e.g., CVE-2019-9232, CVE-2019-9250) \n",
    "- Some clusters miss `cwe` and `cpe` information (e.g., CVE-2019-0235, CVE-2019-15874, CVE-2019-16011)\n",
    "- Some vulns distinguish themselves explicitly in the description using other CVE-IDs, while the machine readable information remain the same (e.g. 'CVE-2019-0889', 'CVE-2019-0890', 'CVE-2019-0891', 'CVE-2019-0893', 'CVE-2019-0894', 'CVE-2019-0895', 'CVE-2019-0896', 'CVE-2019-0897', 'CVE-2019-0898', 'CVE-2019-0899', 'CVE-2019-0900', 'CVE-2019-0901', 'CVE-2019-1146', 'CVE-2019-1147', 'CVE-2019-1156', 'CVE-2019-1157', 'CVE-2019-1240', 'CVE-2019-1241', 'CVE-2019-1242', 'CVE-2019-1243', 'CVE-2019-1247', 'CVE-2019-1248', 'CVE-2019-1249', 'CVE-2019-1250', 'CVE-2019-1358', 'CVE-2019-1359'). Moreover these vulns use microsoft operating systems as product, while the vulnerability resides in the Windows Jet Database Engine.\n",
    "- Some vulnerabilities are batch-added (CVE-2019-7762, CVE-2019-7763, CVE-2019-7764, CVE-2019-7765, CVE-2019-7766, CVE-2019-7767, CVE-2019-7768, CVE-2019-7772, CVE-2019-7781, CVE-2019-7782, CVE-2019-7783, CVE-2019-7788, CVE-2019-7791, CVE-2019-7792, CVE-2019-7805, CVE-2019-7806, CVE-2019-7807, CVE-2019-7808, CVE-2019-7832, CVE-2019-7833, CVE-2019-7834, CVE-2019-7835)\n",
    "- In its current form, the CPE is hardy used for its full potential (CVE-2019-8010: `cpe:2.3:a:adobe:acrobat_reader_dc:*:*:*:*:classic:*:*:*` misses the versions)\n",
    "- Old CVEs (especially in 2002) omit specific `cwe` classes and use `NVD-CWE-Other` instead\n",
    "\n",
    "1: https://nvd.nist.gov/products/cpe/search/results?namingFormat=2.3&keyword=libvpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import plotting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Plot\n",
    "\n",
    "Use the `data` dict to prepare plotting data.\n",
    "Define before solving data `old` and after solving data `new`.\n",
    "Adjust scales `max_old, max_new, max_all` and grab colormap and normalization function.\n",
    "\n",
    "### Plot data for original data\n",
    "\n",
    "Iterate over the size dicts and create for each possible clustersize a list with the sizes per year, e.g. `[0] * len(years)` would be result for the clustersize `0` since for all clusters `c` `len(c) > 0`. This analysis starts with clustersize 2, because clustersize 1 would distort the plot to much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xticklabels = [label if int(label) % 5 == 0 else '' for label in original_db]\n",
    "max_cluster_size_original = max(size for year in original_cluster_size_dict for size in original_cluster_size_dict[year].keys())\n",
    "max_cluster_size_solved = max(size for year in solved_cluster_size_dict for size in solved_cluster_size_dict[year].keys())\n",
    "max_all = max(max_cluster_size_original, max_cluster_size_solved)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(xticklabels))  # label locations\n",
    "width = 0.3  # the width of the bars\n",
    "\n",
    "# colormap and normalization function\n",
    "cmap = cm.get_cmap('tab20c')\n",
    "norm = colors.LogNorm(vmin=1, vmax=max_all)\n",
    "\n",
    "# plot original cluster sizes\n",
    "before_original = np.zeros(len(original_cluster_size_dict))\n",
    "for cluster_size in range(2, max_cluster_size_original + 1):\n",
    "    data = [len(size_dict.get(cluster_size, []))*cluster_size for size_dict in original_cluster_size_dict.values()]\n",
    "    if any(data):\n",
    "        ax.bar(x - width / 2, data, width, bottom=before_original, color=cmap(cluster_size))\n",
    "        before_original += data\n",
    "        \n",
    "        \n",
    "# plot solved cluster sizes\n",
    "before_solved = np.zeros(len(solved_cluster_size_dict))\n",
    "for cluster_size in range(2, max_cluster_size_solved + 1):\n",
    "    data = [len(size_dict.get(cluster_size, []))*cluster_size for size_dict in solved_cluster_size_dict.values()]\n",
    "    if any(data):\n",
    "        ax.bar(x + width / 2, data, width, bottom=before_solved, color=cmap(cluster_size))\n",
    "        before_solved += data\n",
    "        \n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Numbers of clusters')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(xticklabels)\n",
    "\n",
    "plt.grid(linestyle='-', linewidth=2, axis='y', alpha=0.15)\n",
    "\n",
    "fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax, label='Clustersize')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "f = open(\"../dataset/all_cves.json\", \"w\")\n",
    "for year in original_db:\n",
    "    for entry in original_db[year]:\n",
    "        sentence = Sentence(entry.description[0], use_tokenizer=True)\n",
    "        for token in (token.text for token in sentence):\n",
    "            f.write(token + \" \" + entry.id + '\\n')         \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ovana",
   "language": "python",
   "name": "ovana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
