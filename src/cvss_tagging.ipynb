{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20ltO8cm4Dla"
   },
   "source": [
    "# OVANA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this notebook in Google Colab and provide it a Google Drive with the files stored in `../dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXgycJaxcePK"
   },
   "source": [
    "Switch runtime to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nybGDh3V43aI"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, TransformerWordEmbeddings, StackedEmbeddings, FastTextEmbeddings, CharacterEmbeddings, DocumentLSTMEmbeddings\n",
    "from flair.models import SequenceTagger, TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus, CSVClassificationCorpus\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTyUFGB1bcyV"
   },
   "source": [
    "Start this notebook from the following path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZuTvF_Taiwe"
   },
   "outputs": [],
   "source": [
    "path = \"/content/drive/My Drive/Colab Notebooks/OVANA/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m196SOLHaUwu",
    "outputId": "8bfabca0-c4ff-4893-95c3-6c77ec29e222"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6TWtWDO3u9Y"
   },
   "source": [
    "# [First Use Only] Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ygEJaWjdCNi"
   },
   "source": [
    "Save the tagged CVEs into path/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wNUwEy7KF6Z"
   },
   "outputs": [],
   "source": [
    "years = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"additional_1\", \"additional_2\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa_14pKpI4ki"
   },
   "outputs": [],
   "source": [
    "new_file = open(\"tagged_all.csv\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDithBu_PdB8"
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    with open(path + \"dataset/tagged_descriptions_\" + year +\".json\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            word_line = line.split(\" \")\n",
    "            \n",
    "            if i != 0 and last_cve_id != word_line[2].replace(\"\\n\", \"\"):\n",
    "                new_file.write(\"\\n\")\n",
    "            last_cve_id = word_line[2].replace(\"\\n\", \"\")\n",
    "            \n",
    "            new_file.write(\" \".join(word_line))\n",
    "    new_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6WE16tkJW9x"
   },
   "outputs": [],
   "source": [
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tfg947cD3YuS"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "data_tmp = pd.read_table(\"tagged_all.csv\", sep=\" \", encoding=\"utf-8\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None, na_values=[\"NaN\", \"\"], keep_default_na=False)\n",
    "data_tmp[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c91IUwzfQQOQ"
   },
   "source": [
    "Calculating where to split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcSLQUb0Swwt"
   },
   "outputs": [],
   "source": [
    "dev_size = 0.1\n",
    "test_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Vn4r0e8NyGv"
   },
   "outputs": [],
   "source": [
    "items_rows = [i for i, n in enumerate(data_tmp.iloc[:,2].tolist()) if str(n) == \"nan\"]\n",
    "train_last_row = items_rows[int(len(items_rows)*(1.0-dev_size-test_size))]\n",
    "dev_last_row = items_rows[int(len(items_rows)*(1.0-test_size))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lbnGSsd1gV1H",
    "outputId": "c2db27b6-c7f0-4dec-a2ca-d268b0d3e77e"
   },
   "outputs": [],
   "source": [
    "len(items_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nwy45qX4ZfL"
   },
   "source": [
    "Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAW7MMx94VTC"
   },
   "outputs": [],
   "source": [
    "train_set = data_tmp.iloc[0:train_last_row]\n",
    "train_set.to_csv('train.csv', sep=' ', index = False, header = False, quoting=csv.QUOTE_NONE)\n",
    "dev_set = data_tmp.iloc[train_last_row:dev_last_row]\n",
    "dev_set.to_csv('dev.csv', sep=' ', index = False, header = False, quoting=csv.QUOTE_NONE)\n",
    "test_set = data_tmp.iloc[dev_last_row:]\n",
    "test_set.to_csv('test.csv', sep=' ', index = False, header = False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA2qxridrfVo"
   },
   "source": [
    "#Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYz17lCNdSXg"
   },
   "source": [
    "Name the tag that should be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEJYZkuEO9Pe"
   },
   "outputs": [],
   "source": [
    "tag_to_be_predicted = \"AI\"\n",
    "oversample = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6oSu21NS8eK"
   },
   "outputs": [],
   "source": [
    "def oversample(tag, set, n):\n",
    "    #new_file = open(set + \"_oversampled.csv\", \"w\")\n",
    "\n",
    "    with open(set + \".csv\", \"r\") as file:\n",
    "        data = file.readlines()\n",
    "    file.close()\n",
    "\n",
    "    current_instance = []\n",
    "    has_tag = False\n",
    "    for i, line in enumerate(data):\n",
    "        if line.replace(\" \", \"\") == \"\\n\":\n",
    "            if has_tag:\n",
    "                for j in range(n):\n",
    "                    for (word, t, e) in current_instance:\n",
    "                        data[i] += word\n",
    "                        data[i] += \" \" + t\n",
    "                        data[i] += \" \" + e\n",
    "                    data[i] += \"  \\n\"\n",
    "            has_tag = False\n",
    "            current_instance = []\n",
    "            continue\n",
    "\n",
    "        word_line = line.split(\" \")\n",
    "        if tag in word_line[1]:\n",
    "            has_tag = True\n",
    "            \n",
    "        current_instance.append((word_line[0], word_line[1], word_line[2]))\n",
    "\n",
    "    with open(set + \"_oversampled.csv\", \"w\") as file:\n",
    "        file.writelines(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rkf0S9LvdfJk"
   },
   "source": [
    "Specify if the dataset examples containing the tag should be oversampled (do not execute if no oversampling should be performed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71R4DRmhdvuC"
   },
   "outputs": [],
   "source": [
    "oversample(tag_to_be_predicted, \"train\", 2)\n",
    "oversample = \"_oversampled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl9sRdCP364T"
   },
   "source": [
    "# Converting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaQ1CJyy4Gcc"
   },
   "source": [
    "Upload the right train.csv, dev.csv and test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_5cQRO6eKWb"
   },
   "source": [
    "Normal tagging scheme used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZlAaGAi4SvO"
   },
   "outputs": [],
   "source": [
    "# Normal\n",
    "def creat_tagging_dataset_for(tag, set):\n",
    "    new_file = open(set + \"_tagging_\" + tag + \".csv\", \"w\")\n",
    "\n",
    "    with open(set + \".csv\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                new_file.write(\"\\n\")\n",
    "                continue\n",
    "\n",
    "            word_line = line.split(\" \")\n",
    "            \n",
    "            if tag in word_line[1]:\n",
    "                word_line[1] = tag\n",
    "            else:\n",
    "                word_line[1] = \"O\"\n",
    "            new_file.write(\" \".join(word_line))\n",
    "\n",
    "    new_file.close()\n",
    "\n",
    "creat_tagging_dataset_for(tag_to_be_predicted, \"train\" + oversample)\n",
    "creat_tagging_dataset_for(tag_to_be_predicted, \"dev\")\n",
    "creat_tagging_dataset_for(tag_to_be_predicted, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaFyNuffeGY3"
   },
   "source": [
    "Uncomment and execute for BIO tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuLWzS-J1gtE"
   },
   "outputs": [],
   "source": [
    "#def creat_tagging_dataset_for(tag, set):\n",
    "    new_file = open(set + \"_tagging_\" + tag + \".csv\", \"w\")\n",
    "    \n",
    "    is_begin = True\n",
    "    with open(set + \".csv\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                new_file.write(\"\\n\")\n",
    "                continue\n",
    "\n",
    "            word_line = line.split(\" \")\n",
    "            \n",
    "            if tag in word_line[1]:\n",
    "                word_line[1] = \"B\" if is_begin else \"I\"\n",
    "                is_begin = False\n",
    "            else:\n",
    "                word_line[1] = \"O\"\n",
    "                is_begin = True\n",
    "            new_file.write(\" \".join(word_line))\n",
    "\n",
    "    new_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t-bIyuQeAXe"
   },
   "source": [
    "Uncomment and execute for Dong et al. like tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpXeVM56zByN"
   },
   "outputs": [],
   "source": [
    "#def creat_tagging_dataset_for(tag, set):\n",
    "    new_file = open(set + \"_tagging_\" + tag + \".csv\", \"w\")\n",
    "\n",
    "    with open(set + \".csv\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                new_file.write(\"\\n\")\n",
    "                continue\n",
    "\n",
    "            word_line = line.split(\" \")\n",
    "            \n",
    "            if tag in word_line[1]:\n",
    "                word_line[1] = tag + \"\\n\"\n",
    "            else:\n",
    "                word_line[1] = \"O\\n\"\n",
    "            new_file.write(\" \".join(word_line))\n",
    "\n",
    "    new_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV97X1sn4nlS"
   },
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JT47aDgrLBo0"
   },
   "outputs": [],
   "source": [
    "corpus = ColumnCorpus(Path('./'), {0: 'text', 1: 'tag'},\n",
    "                                test_file=\"test\" + \"_tagging_\" + tag_to_be_predicted + \".csv\",\n",
    "                                dev_file=\"dev\" + \"_tagging_\" + tag_to_be_predicted + \".csv\",\n",
    "                                train_file=\"train\" + oversample + \"_tagging_\" + tag_to_be_predicted + \".csv\")\n",
    "\n",
    "tag_type = \"tag\"\n",
    "\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(corpus.train[0].to_tagged_string('tag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKJrE60Zxs1F"
   },
   "outputs": [],
   "source": [
    "print(corpus.train[1].to_tagged_string('tag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QRxO8PtbzHjq"
   },
   "outputs": [],
   "source": [
    "print(corpus.train[59].to_tagged_string('tag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ez721ALGT3Yx"
   },
   "outputs": [],
   "source": [
    "print(len(corpus.train))\n",
    "print(len(corpus.dev))\n",
    "print(len(corpus.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNKOydDPRP8L"
   },
   "outputs": [],
   "source": [
    "corpus.dev[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLEKHrFKy6U0"
   },
   "outputs": [],
   "source": [
    "corpus.test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwBSBw-ZfYhl"
   },
   "source": [
    "# CVE Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAENZmbz4Yxu"
   },
   "outputs": [],
   "source": [
    "bert_embedding = TransformerWordEmbeddings('bert-base-cased', fine_tune=True)\n",
    "word_embeddings = [FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast'), bert_embedding]\n",
    "embeddings = StackedEmbeddings(embeddings=word_embeddings)\n",
    " \n",
    "\n",
    "tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=tag_dictionary,\n",
    "                        tag_type=tag_type,\n",
    "                        use_crf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJwM8G6T4xok"
   },
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vo4wfddhEjDr"
   },
   "outputs": [],
   "source": [
    "from torch.optim.adam import Adam\n",
    "trainer = ModelTrainer(tagger, corpus)#, optimizer=Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CcSUOYPBEem-"
   },
   "outputs": [],
   "source": [
    "#learning_rate_tsv = trainer.find_learning_rate('./')\n",
    "\n",
    "#from flair.visual.training_curves import Plotter\n",
    "#plotter = Plotter()\n",
    "#plotter.plot_learning_rate(learning_rate_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nC0RK5biFioF"
   },
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPAxyuA44zix"
   },
   "outputs": [],
   "source": [
    "trainer.train('./', max_epochs=20, embeddings_storage_mode=\"cpu\", param_selection_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfRn70jWhJPI"
   },
   "outputs": [],
   "source": [
    "result, score = tagger.evaluate(corpus.train, mini_batch_size=32) #, out_path=f\"predictions.txt\")\n",
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XufBdHAmA4OU"
   },
   "outputs": [],
   "source": [
    "result, score = tagger.evaluate(corpus.dev, mini_batch_size=32) #, out_path=f\"predictions.txt\")\n",
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHQ9bURuDhYU"
   },
   "outputs": [],
   "source": [
    "result, score = tagger.evaluate(corpus.test, mini_batch_size=32) #, out_path=f\"predictions.txt\")\n",
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKO9hpdlfKbE"
   },
   "source": [
    "Prediction of the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_NdvuJ2G_8h"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for part in range(1,3):\n",
    "    new_file = open(\"all_cves_\" + str(part) + \".csv\", \"w\")\n",
    "    with open(path + \"dataset/all_cves_\" + str(part) + \".json\", \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            word_line = line.split(\" \")\n",
    "            if len(word_line) < 2:\n",
    "                continue\n",
    "                \n",
    "            if i != 0 and last_cve_id != word_line[1].replace(\"\\n\", \"\"):\n",
    "                new_file.write(\"\\n\")\n",
    "            last_cve_id = word_line[1].replace(\"\\n\", \"\")\n",
    "                \n",
    "            new_file.write(\" \".join(word_line))\n",
    "    new_file.write(\"\\n\")\n",
    "    new_file.close()\n",
    "\n",
    "    cves_tmp = pd.read_table(\"all_cves_\" + str(part) + \".csv\", sep=\" \", encoding=\"utf-8\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None, na_values=[\"NaN\", \"\"], keep_default_na=False)\n",
    "    cves_tmp.to_csv(\"all_cves_\" + str(part) + \"_converted.csv\", sep=' ', index = False, header = False, quoting=csv.QUOTE_NONE)\n",
    "   \n",
    "    cves_tmp = None\n",
    "\n",
    "    \n",
    "    from flair.datasets import ColumnDataset\n",
    "    p = Path('./') / (\"all_cves_\" + str(part) + \"_converted.csv\")\n",
    "    corpus_to_be_labeled = ColumnDataset(p, {0: 'text'})\n",
    "\n",
    "\n",
    "    tagger.predict(sentences=corpus_to_be_labeled, mini_batch_size=16)\n",
    "\n",
    "\n",
    "    new_file = open(\"all_cves_tagged_\" + str(part) + \"_\" + tag_to_be_predicted + \".csv\", \"w\")\n",
    "    for sentence in corpus_to_be_labeled.sentences:\n",
    "        sentence_with_tags = sentence.to_tagged_string('tag')\n",
    "        tokenized_sentence = sentence_with_tags.split(\" \")\n",
    "        for i,word in enumerate(tokenized_sentence):\n",
    "            if word == \"<\" + tag_to_be_predicted + \">\":\n",
    "                continue\n",
    "            new_file.write(word)\n",
    "\n",
    "            if i == len(tokenized_sentence) - 1:\n",
    "                new_file.write(\" O\\n\")\n",
    "                continue\n",
    "\n",
    "            if tokenized_sentence[i+1] == \"<\" + tag_to_be_predicted + \">\":\n",
    "                new_file.write(\" \" + tag_to_be_predicted + \"\\n\")\n",
    "            else:\n",
    "                new_file.write(\" O\\n\")\n",
    "        new_file.write(\"  \\n\")\n",
    "\n",
    "    new_file.close()\n",
    "    corpus_to_be_labeled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4TWBE3Ivrx0"
   },
   "outputs": [],
   "source": [
    "#import csv\n",
    "#import random\n",
    "\n",
    "#for part in range(1,3):\n",
    "#    new_file = open(\"all_cves_\" + str(part) + \".csv\", \"w\")\n",
    "#    with open(path + \"dataset/all_cves_\" + str(part) + \".json\", \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "#        for i, line in enumerate(file):\n",
    "#            word_line = line.split(\" \")\n",
    "#            if len(word_line) < 2:\n",
    "#                continue\n",
    "#                \n",
    "#            if i != 0 and last_cve_id != word_line[1].replace(\"\\n\", \"\"):\n",
    "#                new_file.write(\"\\n\")\n",
    "#            last_cve_id = word_line[1].replace(\"\\n\", \"\")\n",
    "#                \n",
    "#            new_file.write(\" \".join(word_line))\n",
    "#    new_file.write(\"\\n\")\n",
    "#    new_file.close()\n",
    "#\n",
    "#    cves_tmp = pd.read_table(\"all_cves_\" + str(part) + \".csv\", sep=\" \", encoding=\"utf-8\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None, na_values=[\"NaN\", \"\"], keep_default_na=False)\n",
    "#\n",
    "#    cves_tmp.to_csv(\"all_cves_\" + str(part) + \"_converted.csv\", sep=' ', index = False, header = False, quoting=csv.QUOTE_NONE)\n",
    "#   \n",
    "#    cves_tmp = None\n",
    "#\n",
    "#    \n",
    "#    from flair.datasets import ColumnDataset\n",
    "#    p = Path('./') / (\"all_cves_\" + str(part) + \"_converted.csv\")\n",
    "#    corpus_to_be_labeled = ColumnDataset(p, {0: 'text'})\n",
    "#\n",
    "#    new_file = open(\"all_cves_tagged_\" + str(part) + \"_\" + tag_to_be_predicted + \".csv\", \"w\")\n",
    "#    for sentence in corpus_to_be_labeled.sentences:\n",
    "#        sentence_with_tags = sentence.to_tagged_string('tag')\n",
    "#        tokenized_sentence = sentence_with_tags.split(\" \")\n",
    "#        for i,word in enumerate(tokenized_sentence):\n",
    "#            if word == \"<\" + tag_to_be_predicted + \">\":\n",
    "#                continue\n",
    "#            new_file.write(word)\n",
    "#\n",
    "#            #if i == len(tokenized_sentence) - 1:\n",
    "#            #    new_file.write(\" O\\n\")\n",
    "#            #    continue\n",
    "#\n",
    "#            if bool(random.getrandbits(1)):\n",
    "#                new_file.write(\" \" + tag_to_be_predicted + \"\\n\")\n",
    "#            else:\n",
    "#                new_file.write(\" O\\n\")\n",
    "#        new_file.write(\"  \\n\")\n",
    "\n",
    "#    new_file.close()\n",
    "#    corpus_to_be_labeled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aav2MMcxwGa"
   },
   "outputs": [],
   "source": [
    "tagged_1_name = \"all_cves_tagged_1_\" + tag_to_be_predicted +  \".csv\"\n",
    "tagged_2_name = \"all_cves_tagged_2_\" + tag_to_be_predicted +  \".csv\"\n",
    "!cp \"$tagged_1_name\" \"$path\"\n",
    "!cp \"$tagged_2_name\" \"$path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6QGaFUJwWRv"
   },
   "outputs": [],
   "source": [
    "tagger = None\n",
    "trainer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um1AUdl8n5Fp"
   },
   "source": [
    "# Value Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0uhqy6mmdQz"
   },
   "source": [
    "## CVSS-Matcher integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZsCqm7s7fbf"
   },
   "outputs": [],
   "source": [
    "years = [\"2013\", \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"additional_1\", \"additional_2\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtZOvHYS7gJO"
   },
   "outputs": [],
   "source": [
    "new_file = open(\"tagged_all.csv\", \"w\")\n",
    "for year in years:\n",
    "    with open(path + \"dataset/tagged_descriptions_\" + year +\".json\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            word_line = line.split(\" \")\n",
    "            \n",
    "            if i != 0 and last_cve_id != word_line[2].replace(\"\\n\", \"\"):\n",
    "                new_file.write(\"\\n\")\n",
    "            last_cve_id = word_line[2].replace(\"\\n\", \"\")\n",
    "            \n",
    "            new_file.write(\" \".join(word_line))\n",
    "    new_file.write(\"\\n\")\n",
    "new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AwGJ6EBpONve"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "tagged_all_tmp = pd.read_table(\"tagged_all.csv\", sep=\" \", encoding=\"utf-8\", quoting=csv.QUOTE_NONE, skip_blank_lines=False, header=None, na_values=[\"NaN\", \"\"], keep_default_na=False)\n",
    "tagged_all_tmp.to_csv('tagged_all_tmp.csv', sep=' ', index = False, header = False, quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfxYy21Rfro7"
   },
   "source": [
    "Save Matcher in path/matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPdK2TbzfvE0"
   },
   "outputs": [],
   "source": [
    "matcher_path = path + \"matcher/\"\n",
    "dataset_path = path + \"dataset/\"\n",
    "!cp tagged_all_tmp.csv \"$dataset_path\"\n",
    "%cd \"$matcher_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8ycOff4M6Xq"
   },
   "outputs": [],
   "source": [
    "matcher_path = path + \"matcher/cvss_matcher.py\"\n",
    "output_path = \"../dataset/tagged_all_values_\" + tag_to_be_predicted + \".csv\"\n",
    "!python \"cvss_matcher.py\" $tag_to_be_predicted \"../dataset/tagged_all_tmp.csv\" \"$output_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1HujRXklQKT"
   },
   "outputs": [],
   "source": [
    "%cd \n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usnvRZHRp-p4"
   },
   "outputs": [],
   "source": [
    "train_file = dataset_path + \"tagged_all_values_\" + tag_to_be_predicted + \".csv\"\n",
    "!cp \"$train_file\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiibnCzi933S"
   },
   "outputs": [],
   "source": [
    "def create_classification_dataset_for(tag, set):\n",
    "    new_file = open(set + \"_classification_\" + tag + \".csv\", \"w\")\n",
    "    \n",
    "    label = \"\"\n",
    "    with open(set + \"_values_\" + tag + \".csv\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                if label != \"\":\n",
    "                    new_file.write(\"\\n\")\n",
    "                    new_file.write(\"\\n\")\n",
    "                label = \"\"\n",
    "                continue\n",
    "            word_line = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            \n",
    "            if tag in word_line[1]:\n",
    "                if \":\" not in word_line[1]:\n",
    "                    continue\n",
    "                if label == \"\":\n",
    "                    label = word_line[1].split(\":\")[1]\n",
    "                    new_file.write(label + \" \\t\")\n",
    "                new_file.write(word_line[0] + \" \")\n",
    "\n",
    "    new_file.close()\n",
    "\n",
    "create_classification_dataset_for(tag_to_be_predicted, \"tagged_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gYCVOsEL7Us"
   },
   "outputs": [],
   "source": [
    "dev_size = 0.1\n",
    "test_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGtPjzhb-ms6"
   },
   "outputs": [],
   "source": [
    "def split_dataset(set):\n",
    "    class_dict = {}\n",
    "    with open(set + \".csv\", \"r\") as file:\n",
    "        for line in file:\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                continue\n",
    "            words = line.split(\" \")\n",
    "            value = words[0]  \n",
    "            if value in class_dict:\n",
    "                class_dict[value].append(line)\n",
    "            else:\n",
    "                class_dict[value] = [line]\n",
    "\n",
    "    train_set, dev_set, test_set = [],[],[]\n",
    "    for key in class_dict.keys():\n",
    "        value_data = class_dict[key]\n",
    "        train_set.extend(value_data[:int(len(value_data)*(1.0-dev_size-test_size))])\n",
    "        dev_set.extend(value_data[int(len(value_data)*(1.0-dev_size-test_size)):int(len(value_data)*(1.0-dev_size))])\n",
    "        test_set.extend(value_data[int(len(value_data)*(1.0-dev_size)):])\n",
    "\n",
    "    new_file = open(\"train_classification_\" + tag_to_be_predicted + \".csv\", \"w\")\n",
    "    for entity in train_set:\n",
    "        new_file.write(entity)\n",
    "        new_file.write(\"\\n\")\n",
    "    new_file.close()\n",
    "    new_file = open(\"dev_classification_\" + tag_to_be_predicted + \".csv\", \"w\")\n",
    "    for entity in dev_set:\n",
    "        new_file.write(entity)\n",
    "        new_file.write(\"\\n\")\n",
    "    new_file.close()\n",
    "    new_file = open(\"test_classification_\" + tag_to_be_predicted + \".csv\", \"w\")\n",
    "    for entity in test_set:\n",
    "        new_file.write(entity)\n",
    "        new_file.write(\"\\n\")\n",
    "    new_file.close()\n",
    "\n",
    "split_dataset(\"tagged_all_classification_\" + tag_to_be_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLmX-rRv-09G"
   },
   "outputs": [],
   "source": [
    "oversample = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCAuIlfo10dr"
   },
   "outputs": [],
   "source": [
    "def oversample_classification(class_to_be_oversampled, set, n):\n",
    "    with open(set + \".csv\", \"r\") as file:\n",
    "        data = file.readlines()\n",
    "    file.close()\n",
    "\n",
    "    has_class = False\n",
    "    for i, line in enumerate(data):\n",
    "        word_line = line.split(\" \")\n",
    "        if class_to_be_oversampled in word_line[0]:\n",
    "            for j in range(n):\n",
    "                data[i] += \"\\n\"\n",
    "                data[i] += line\n",
    "\n",
    "    with open(set + \"_oversampled.csv\", \"w\") as file:\n",
    "        file.writelines(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1frm3gb5f5qX"
   },
   "source": [
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JIPD7Xuf7Wa"
   },
   "outputs": [],
   "source": [
    "oversample_classification(\"H\", \"train_classification_\" + tag_to_be_predicted, 15)\n",
    "oversample = \"_oversampled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Ef9ggXygC2C"
   },
   "source": [
    "Corpus creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SWvon0bGz_Z"
   },
   "outputs": [],
   "source": [
    "corpus = CSVClassificationCorpus(Path('./'), {1: 'text', 0: 'label'}, delimiter= \"\\t\", \n",
    "                                test_file=\"test\" + \"_classification_\" + tag_to_be_predicted + \".csv\",\n",
    "                                dev_file=\"dev\" + \"_classification_\" + tag_to_be_predicted + \".csv\",\n",
    "                                train_file=\"train\" + \"_classification_\" + tag_to_be_predicted + oversample + \".csv\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "print(corpus.train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hYxzMRfLID7l"
   },
   "outputs": [],
   "source": [
    "print(len(corpus.train))\n",
    "print(len(corpus.dev))\n",
    "print(len(corpus.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ksF8_oyLn4WD"
   },
   "outputs": [],
   "source": [
    "bert_embedding = TransformerWordEmbeddings('bert-base-cased', fine_tune=True)\n",
    "word_embeddings = [FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast'), bert_embedding]\n",
    "document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K31dFMKuDyo8"
   },
   "outputs": [],
   "source": [
    "#learning_rate_tsv = trainer.find_learning_rate('./')\n",
    "\n",
    "#from flair.visual.training_curves import Plotter\n",
    "#plotter = Plotter()\n",
    "#plotter.plot_learning_rate(learning_rate_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2HaalEKIIJA"
   },
   "outputs": [],
   "source": [
    "trainer = ModelTrainer(classifier, corpus)\n",
    "trainer.train('./', max_epochs=20, embeddings_storage_mode=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KGKy_LlYTxii"
   },
   "outputs": [],
   "source": [
    "result, score = classifier.evaluate(corpus.train, mini_batch_size=32) #, out_path=f\"predictions.txt\")\n",
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubuvwWIaNAi1"
   },
   "outputs": [],
   "source": [
    "result, score = classifier.evaluate(corpus.dev, mini_batch_size=32) #, out_path=f\"predictions.txt\")\n",
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSR71jh1vWwB"
   },
   "outputs": [],
   "source": [
    "result, score = classifier.evaluate(corpus.dev, mini_batch_size=32) #, out_path=f\"predictions.txt\")\n",
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pHk_8jU_GzU"
   },
   "outputs": [],
   "source": [
    "result, score = classifier.evaluate(corpus.test, mini_batch_size=32) #, out_path=f\"predictions.txt\")\n",
    "print(result.detailed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xz-dPZo-g4bR"
   },
   "source": [
    "Predict the values of the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_BEAw1MEC4dj"
   },
   "outputs": [],
   "source": [
    "def create_unlabeled_classification_dataset_for(tag, set):\n",
    "    new_file = open(set + \"_classification_\" + tag + \".csv\", \"w\")\n",
    "    \n",
    "    label = \"\"\n",
    "    with open(set + \"_\" + tag + \".csv\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                if label != \"\":\n",
    "                    new_file.write(\"\\t\")\n",
    "                    new_file.write(\"\\n\")\n",
    "                    new_file.write(\"\\n\")\n",
    "                label = \"\"\n",
    "                continue\n",
    "            word_line = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            if len(word_line) < 2:\n",
    "                print(word_line)\n",
    "            if tag in word_line[1]:\n",
    "                if label == \"\":\n",
    "                    label = \"X\"\n",
    "                    new_file.write(label + \" \\t\")\n",
    "                new_file.write(word_line[0] + \" \")\n",
    "\n",
    "    new_file.close()\n",
    "\n",
    "i = 0\n",
    "for part in range(1,3):\n",
    "    create_unlabeled_classification_dataset_for(tag_to_be_predicted, \"all_cves_tagged_\" + str(part))\n",
    "\n",
    "\n",
    "    from flair.datasets import CSVClassificationDataset\n",
    "    name = \"all_cves_tagged_\" + str(part) + \"_classification_\" + tag_to_be_predicted + \".csv\"\n",
    "    corpus_to_be_classified = CSVClassificationDataset(Path('.') / name, {0: 'label', 1: 'text'}, delimiter= \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "    classifier.predict(sentences=corpus_to_be_classified, mini_batch_size=1)\n",
    "        \n",
    "        \n",
    "    new_file = open(\"all_cves_tagged_classified_\" + str(part) + \"_\" + tag_to_be_predicted + \".csv\", \"w\")\n",
    "    j = 0\n",
    "    with open(\"all_cves_tagged_\" + str(part) + \"_\" + tag_to_be_predicted + \".csv\", \"r\") as file:\n",
    "        for i in range(len(corpus_to_be_classified)):\n",
    "            for word in corpus_to_be_classified.sentences[i].to_plain_string().split(\" \"):\n",
    "                for j, line in enumerate(file):\n",
    "                    if line.replace(\" \", \"\") == \"\\n\":\n",
    "                        new_file.write(line)\n",
    "                        continue\n",
    "                        \n",
    "                    ref_word, tag = line.replace(\"\\n\", \"\").split(\" \")\n",
    "\n",
    "                    new_file.write(ref_word)\n",
    "                    new_file.write(\" \" + tag)\n",
    "\n",
    "\n",
    "                    if word.replace(\" \", \"\") == ref_word.replace(\" \", \"\") and tag.replace(\" \", \"\") == tag_to_be_predicted:\n",
    "                        new_file.write(\":\")\n",
    "                        new_file.write(corpus_to_be_classified.sentences[i].get_label_names()[1].replace(\" \", \"\"))\n",
    "                        new_file.write(\"\\n\")\n",
    "                        break\n",
    "                    new_file.write(\"\\n\")\n",
    "\n",
    "        for line in file:\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                    new_file.write(line)\n",
    "                    continue\n",
    "            ref_word, tag = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            new_file.write(ref_word)\n",
    "            new_file.write(\" \" + tag)\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "    new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CyaYPtAbw-IB"
   },
   "outputs": [],
   "source": [
    "tagged_classified_1_name = \"all_cves_tagged_classified_1_\" + tag_to_be_predicted + \".csv\"\n",
    "tagged_classified_2_name = \"all_cves_tagged_classified_2_\" + tag_to_be_predicted + \".csv\"\n",
    "!cp \"$tagged_classified_1_name\" \"$path\"\n",
    "!cp \"$tagged_classified_2_name\" \"$path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcK3HNTIw12x"
   },
   "outputs": [],
   "source": [
    "classifier = None\n",
    "trainer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9SzoXQOk_m2"
   },
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1E4FfcDh_AUg"
   },
   "outputs": [],
   "source": [
    "def create_unlabeled_classification_dataset_for(tag, set):\n",
    "    new_file = open(set + \"_classification_\" + tag + \".csv\", \"w\")\n",
    "    \n",
    "    label = \"\"\n",
    "    with open(set + \"_\" + tag + \".csv\", \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                if i == 0:\n",
    "                    continue\n",
    "                if label != \"\":\n",
    "                    new_file.write(\"\\t\")\n",
    "                    new_file.write(\"\\n\")\n",
    "                    new_file.write(\"\\n\")\n",
    "                label = \"\"\n",
    "                continue\n",
    "            word_line = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            \n",
    "            if tag in word_line[1]:\n",
    "                if label == \"\":\n",
    "                    label = \"X\"\n",
    "                    new_file.write(label + \" \\t\")\n",
    "                new_file.write(word_line[0] + \" \")\n",
    "\n",
    "    new_file.close()\n",
    "\n",
    "i = 0\n",
    "for part in range(1,3):\n",
    "    create_unlabeled_classification_dataset_for(tag_to_be_predicted, \"all_cves_tagged_\" + str(part))\n",
    "\n",
    "\n",
    "    from flair.datasets import CSVClassificationDataset\n",
    "    name = \"all_cves_tagged_\" + str(part) + \"_classification_\" + tag_to_be_predicted + \".csv\"\n",
    "    corpus_to_be_classified = CSVClassificationDataset(Path('.') / name, {0: 'label', 1: 'text'}, delimiter= \"\\t\", quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    new_file = open(\"all_cves_tagged_classified_\" + str(part) + \"_\" + tag_to_be_predicted + \".csv\", \"w\")\n",
    "    j = 0\n",
    "    with open(\"all_cves_tagged_\" + str(part) + \"_\" + tag_to_be_predicted + \".csv\", \"r\") as file:\n",
    "        for i in range(len(corpus_to_be_classified)):\n",
    "            for word in corpus_to_be_classified.sentences[i].to_plain_string().split(\" \"):\n",
    "                for j, line in enumerate(file):\n",
    "                    if line.replace(\" \", \"\") == \"\\n\":\n",
    "                        new_file.write(line)\n",
    "                        continue\n",
    "                        \n",
    "                    ref_word, tag = line.replace(\"\\n\", \"\").split(\" \")\n",
    "\n",
    "                    #if i == 159 and j< 100:\n",
    "                    #    print(ref_word)\n",
    "                    #    print(word)\n",
    "                        \n",
    "                    #if j > 20000:\n",
    "                    #    print(word)\n",
    "                    #    word_not_found_error\n",
    "                        \n",
    "                    new_file.write(ref_word)\n",
    "                    new_file.write(\" \" + tag)\n",
    "\n",
    "\n",
    "                    if word.replace(\" \", \"\") == ref_word.replace(\" \", \"\") and tag.replace(\" \", \"\") == tag_to_be_predicted:\n",
    "                        new_file.write(\":\")\n",
    "                        new_file.write(\"N\")\n",
    "                        new_file.write(\"\\n\")\n",
    "                        break\n",
    "                    new_file.write(\"\\n\")\n",
    "\n",
    "        for line in file:\n",
    "            if line.replace(\" \", \"\") == \"\\n\":\n",
    "                    new_file.write(line)\n",
    "                    continue\n",
    "            ref_word, tag = line.replace(\"\\n\", \"\").split(\" \")\n",
    "            new_file.write(ref_word)\n",
    "            new_file.write(\" \" + tag)\n",
    "            new_file.write(\"\\n\")\n",
    "\n",
    "    new_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z63OHvYXjb7O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OVANA_cvss_tagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
