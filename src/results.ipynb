{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from iq.iq import score\n",
    "from nvd_entry import NVDEntry\n",
    "from utils import get_cve_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(scores):\n",
    "    completeness_per_year = {y:d['cm_score'] for y,d in scores.items()}\n",
    "    accuracy_per_year = {y:d['am_score'] for y,d in scores.items()}\n",
    "    uniqueness_per_year = {y:d['um_score'] for y,d in scores.items()}\n",
    "    \n",
    "    return {'Completeness': completeness_per_year, \n",
    "            'Accuracy': accuracy_per_year, \n",
    "            'Uniqueness': uniqueness_per_year}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = (5,8)\n",
    "figheight = figsize[0]\n",
    "figwidth = figsize[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_colors1 = {'Uniqueness': '#3080b8',\n",
    "               'Accuracy': '#ff7f0e',\n",
    "               'Completeness': '#2ca02c'}\n",
    "\n",
    "plot_colors2 = {'Uniqueness': '#1d6496',\n",
    "               'Accuracy': '#ff7f0e',\n",
    "               'Completeness': '#2ca02c'}\n",
    "\n",
    "plot_colors3 = {'Uniqueness': '#0a4670',\n",
    "               'Accuracy': '#ff7f0e',\n",
    "               'Completeness': '#2ca02c'}\n",
    "\n",
    "def get_color(label, index):\n",
    "    if index == 0:\n",
    "        return plot_colors1[label]\n",
    "    if index == 1:\n",
    "        return plot_colors2[label]\n",
    "    if index == 2:\n",
    "        return plot_colors3[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stacked(metrics_list: list):\n",
    "    years = metrics_list[0][[*metrics_list[0].keys()][0]].keys()\n",
    "    xticklabels = [y if int(y) % 5 == 0 else '' for y in years]\n",
    "    x = np.arange(len(xticklabels))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    width = 0.8\n",
    "    \n",
    "    num_metrics = len(metrics_list)\n",
    "\n",
    "    for i, metrics in enumerate(metrics_list):\n",
    "        pos = x - width/num_metrics + i*width/num_metrics + (num_metrics+1)%2 * width/(2*num_metrics) # works now for 2 and 3\n",
    "        offset = np.zeros(len(years))\n",
    "        this_width = width/len(metrics_list)\n",
    "        \n",
    "        for label, metric in metrics.items():\n",
    "            data = list(metric.values())\n",
    "            ax.bar(pos, data, this_width, bottom=offset, label=label, color=get_color(label, i))\n",
    "            offset += data\n",
    "\n",
    "        avg = sum(offset)/len(years)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.legend()\n",
    "    ax.set_ylabel(f'IQ score')\n",
    "    \n",
    "    fig.set_figheight(figheight)\n",
    "    fig.set_figwidth(figwidth)\n",
    "    \n",
    "    plt.grid(linestyle='-', linewidth=1, axis='y', alpha=0.15)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def side_by_side_barplot(metrics: dict, ylabel):\n",
    "    years = metrics[[*metrics.keys()][0]].keys()\n",
    "    num_metrics = len(metrics)\n",
    "    xticklabels = [label if int(label) % 5 == 0 else '' for label in years]\n",
    "    x = np.arange(len(xticklabels))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    width = 0.8\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    for i, (label, metric) in enumerate(metrics.items()):\n",
    "        pos = x - width/num_metrics + i*width/num_metrics + (num_metrics+1)%2 * width/(2*num_metrics) # works now for 2 and 3\n",
    "        data = metric.values()\n",
    "        ax.bar(pos, data, width/num_metrics, label=label)\n",
    "\n",
    "    ax.set_ylabel(f'{ylabel} score')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.grid(linestyle='-', linewidth=1, axis='y', alpha=0.15)\n",
    "    \n",
    "    fig.set_figheight(figheight)\n",
    "    fig.set_figwidth(figwidth)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "def plot_metric(metric: dict, label: str, normalize=False):\n",
    "    xticklabels = [label if int(label) % 5 == 0 else '' for label in metric.keys()]\n",
    "    x = np.arange(len(xticklabels))\n",
    "    if normalize:\n",
    "        norm = Normalize(0, max(metric.values()))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    width = 0.8\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.bar(x, list(map(norm, metric.values())) if normalize else metric.values(), width, color=get_color(label,0))\n",
    "    ax.set_ylabel(f'{label} score{\" normalized\" if normalize else \"\"}')\n",
    "    \n",
    "    avg = sum(metric.values())/len(metric)\n",
    "    print(avg)\n",
    "    ax.axhline(avg, linewidth=1, alpha=0.5, color='black', label='Average')\n",
    "    \n",
    "    plt.grid(linestyle='-', linewidth=1, axis='y', alpha=0.15)\n",
    "    ax.legend()\n",
    "    \n",
    "    fig.set_figheight(figheight)\n",
    "    fig.set_figwidth(figwidth)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_file, get_dict\n",
    "from datetime import datetime\n",
    "\n",
    "dataset_date = max(datetime.strptime(get_dict(get_file(str(y)))['CVE_data_timestamp'], '%Y-%m-%dT%H:%MZ') for y in range(2002, 2020))\n",
    "print(dataset_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nvdfile_data = [NVDEntry(e) for year in range(2002, 2020) for e in get_cve_list(year)]\n",
    "valid_entries = [e for e in nvdfile_data if not e.rejected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_db = defaultdict(list)\n",
    "for e in valid_entries:\n",
    "    original_db[e.year].append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_fields = ['cwe', 'cpe', 'cvssv3']\n",
    "scores_orig = {y:score(original_db[y], structured_fields) for y in original_db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_original = get_metrics(scores_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create updated dataset and store it the same way as `original_db`, i.e., as `dict((int,list[NVDEntry]))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver.sophisticated_updater import parse_files_to_entries, update_db\n",
    "predicted_entries = parse_files_to_entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver.regexsolver import RegexSolver\n",
    "\n",
    "re_solver = RegexSolver()\n",
    "\n",
    "updated_entries = re_solver.solve(valid_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_entries = list(update_db(updated_entries.copy(), predicted_entries))\n",
    "updated_db = defaultdict(list)\n",
    "for e in updated_entries:\n",
    "    updated_db[e.year].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_fields = ['cwe', 'cpe', 'predicted_cvssv3']\n",
    "scores_updated = {y:score(updated_db[y], structured_fields) for y in updated_db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_updated = get_metrics(scores_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_original)\n",
    "print(metrics_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_fields_updated = ['cwe', 'cpe', 'predicted_cvssv3', 'path', 'vulnerable_function']\n",
    "scores_updated_all = {y:score(updated_db[y], structured_fields_updated) for y in updated_db}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_updated_all = get_metrics(scores_updated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeness_all = metrics_updated_all['Completeness']\n",
    "print(sum(completeness_all.values())/len(completeness_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall Preanalysis problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num entries', len(updated_entries))\n",
    "\n",
    "# Problem I    \n",
    "no_cwe = [entry for entry in updated_entries if len(entry.cwe) == 0]\n",
    "cwe_other = [entry for entry in updated_entries if len(entry.cwe) > 0 and any('NVD-CWE' in e for e in entry.cwe)]\n",
    "print(\"No cwe (pI)\\t \", len(cwe_other) + len(no_cwe))\n",
    "\n",
    "# Problem II\n",
    "old_cvss = [entry for entry in updated_entries if not entry.cvssv3 and not entry.predicted_cvssv3]\n",
    "new_cvss = [entry for entry in updated_entries if entry.cvssv3 or entry.predicted_cvssv3]\n",
    "print('Old cvss (pII)\\t ', len(old_cvss))\n",
    "\n",
    "# Problem III\n",
    "path_found = [(entry.path, entry.vulnerable_function) for entry in updated_entries if entry.path or entry.vulnerable_function]\n",
    "print(path_found[:10])\n",
    "print('Path found (pIII)', len(path_found))\n",
    "\n",
    "# Problem IV\n",
    "no_cpe = [entry for entry in updated_entries if len(entry.cpe) == 0]\n",
    "print(\"No cpe (pIV)\\t \", len(no_cpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot information quality\n",
    "Plot it in a way, that the y axis is composed of the different metric values.\n",
    "\n",
    "Beforehand: Move the Accuracy to the original metric to add the diff of the cvss values to the original database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Accuracy' in metrics_updated: \n",
    "    metrics_original['Accuracy'] = metrics_updated['Accuracy']\n",
    "    del metrics_updated['Accuracy']\n",
    "\n",
    "if 'Accuracy' in metrics_updated_all:\n",
    "    del metrics_updated_all['Accuracy']\n",
    "\n",
    "plot_stacked([metrics_original, metrics_updated, metrics_updated_all])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (label, m_orig), (label, m_updated) in zip(metrics_original.items(), metrics_updated.items()):\n",
    "    compared_metrics = {'Original': m_orig, 'Updated: Original keys': m_updated}\n",
    "    side_by_side_barplot(compared_metrics, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stacked([metrics_original, metrics_updated_all])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric(metrics_updated_all['Uniqueness'], 'Uniqueness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing all uniqueness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparing = {\n",
    "    'Orig': metrics_original['Uniqueness'],\n",
    "    'Updated: orig keys': metrics_updated['Uniqueness'],\n",
    "    'Updated: all keys': metrics_updated_all['Uniqueness']}\n",
    "side_by_side_barplot(comparing, 'Uniqueness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iq.uniqueness_metric import UniquenessMetric\n",
    "\n",
    "um = UniquenessMetric(structured_fields)\n",
    "\n",
    "clusters_year = um.build_clusters(original_db[2011])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_cluster = max([(k, len(c)) for k, c in clusters_year.items()], key=lambda i: i[1])\n",
    "\n",
    "print(sorted(e.id for e in clusters_year[biggest_cluster[0]]))\n",
    "print(sorted(e.description for e in clusters_year[biggest_cluster[0]]))\n",
    "print(biggest_cluster[0])\n",
    "print(biggest_cluster[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(1 for e in updated_entries if e.path))\n",
    "print([(e.description, e.path) for e in updated_entries if e.path][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.colors import Normalize\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_iq_comparison(comparing_metrics: list[dict], normalized=False):\n",
    "#     for metrics in comparing_metrics:\n",
    "#         years = metrics[[*metrics.keys()][0]].keys()\n",
    "#         xticklabels = [y if int(y) % 5 == 0 else '' for y in years]\n",
    "#         x = np.arange(len(xticklabels))\n",
    "\n",
    "#         fig, ax = plt.subplots()\n",
    "#         width = 0.8\n",
    "\n",
    "#         offset = np.zeros(len(original_db))\n",
    "\n",
    "#         for label,metric in metrics.items():\n",
    "#             if normalized:\n",
    "#                 norm = Normalize(0, max(metric.values()))\n",
    "#                 data = list(map(norm, metric.values()))\n",
    "#             else:\n",
    "#                 data = list(metric.values())\n",
    "#             ax.bar(x, data, width, bottom=offset, label=label)\n",
    "#             offset += data\n",
    "\n",
    "#         ax.set_xticks(x)\n",
    "#         ax.set_xticklabels(xticklabels)\n",
    "#         ax.legend()\n",
    "\n",
    "#     fig.set_figheight(figheight)\n",
    "#     fig.set_figwidth(figwidth)\n",
    "    \n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attachments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvss.cvssv3 import CVSSV3 \n",
    "\n",
    "translate_values = {\n",
    "    'AV': {'N': 'NETWORK', 'A': 'ADJACENT_NETWORK', 'L': 'LOCAL', 'P': 'PHYSICAL'},\n",
    "    'AC': {'L': 'LOW', 'H': 'HIGH'},\n",
    "    'PR': {'N': 'NONE', 'L': 'LOW', 'H': 'HIGH'},\n",
    "    'UI': {'N': 'NONE', 'R': 'REQUIRED'},\n",
    "    'SC': {'C': 'CHANGED', 'U': 'UNCHANGED'},\n",
    "    'CI': {'H': 'HIGH', 'L': 'LOW', 'N': 'NONE'},\n",
    "    'II': {'H': 'HIGH', 'L': 'LOW', 'N': 'NONE'},\n",
    "    'AI': {'H': 'HIGH', 'L': 'LOW', 'N': 'NONE'}\n",
    "}\n",
    "\n",
    "translate_keys = {\n",
    "    'AV': 'attackVector',\n",
    "    'AC': 'attackComplexity',\n",
    "    'PR': 'privilegesRequired',\n",
    "    'UI': 'userInteraction',\n",
    "    'SC': 'scope',\n",
    "    'CI': 'confidentialityImpact',\n",
    "    'II': 'integrityImpact',\n",
    "    'AI': 'availabilityImpact'\n",
    "}\n",
    "\n",
    "def _get_tags(entry: list):\n",
    "    for l in entry:\n",
    "        if len(l.split()) < 3:\n",
    "            print(l)\n",
    "            continue\n",
    "        token, tags, cve_id = l.split()\n",
    "        for t in tags.split(','):\n",
    "            yield (t,token)\n",
    "\n",
    "def _translate_cvss_tag(tag: str):\n",
    "    parsed = {}\n",
    "    for tag in cvss_tags.split(','):\n",
    "        k,v = tag.split(':')\n",
    "        parsed[translate_keys[k]] = translate_values[k][v]\n",
    "\n",
    "def _parse_for_tags(entry: list):\n",
    "    parsed = defaultdict(list)\n",
    "    for tag, token in _get_tags(entry):\n",
    "        parsed[tag].append(token)\n",
    "    return parsed\n",
    "\n",
    "def _extract_cvss_parameters(parsed_entry: dict):\n",
    "    cvss_parameters = {}\n",
    "    for key in [key for key in parsed_entry if ':' in key]:\n",
    "        param, value = key.split(':')\n",
    "        cvss_parameters[translate_keys[param]] = translate_values[param][value]\n",
    "    return cvss_parameters\n",
    "            \n",
    "def _set_tags(nvd_entry: NVDEntry, parsed: dict):\n",
    "    cvss_parameters = _extract_cvss_parameters(parsed)\n",
    "    \n",
    "    nvd_entry.vulnerable_function = set(parsed.get('VF', []))\n",
    "    nvd_entry.vulnerable_path = set(parsed.get('VP', []))\n",
    "    nvd_entry.weakness = parsed.get('W', [])\n",
    "    nvd_entry.predicted_cvssv3 = CVSSV3(cvss_parameters, nvd_entry.cvssv3.__dict__ if nvd_entry.cvssv3 else None)\n",
    "    \n",
    "def update_db(nvd_dataset: list, predicted_entries: list):\n",
    "    dataset_dict = {e.id:e for e in nvd_dataset}\n",
    "    for e in predicted_entries:\n",
    "        parsed = _parse_for_tags(e)\n",
    "        e_id = e[0].split()[2]\n",
    "        referred = dataset_dict.get(e_id)\n",
    "        if referred:\n",
    "            _set_tags(referred, parsed)\n",
    "            yield referred\n",
    "\n",
    "updated_nvd_entries = list(update_db(valid_entries, predicted_entries))\n",
    "print(len(valid_entries),len(updated_nvd_entries))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ovana",
   "language": "python",
   "name": "ovana"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
